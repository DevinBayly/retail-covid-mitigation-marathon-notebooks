{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home stretch, make data.csv\n",
    "\n",
    "This is a continuation of the previous notebook. So at this point we have gathered pandemic related pages from current retailer websites, then gathered historical snapshots of those pages, and all that remains is to figure out what terms are on these pages. \n",
    "\n",
    "In specific we now have folders in the data_processing directory named after all our retailers. The idea is to peruse these directories and add all file contents to a list that we can then systematically compare to our list of terms. Because there's atleast original 120 links, and 12 months of data for those 120 links, and ~300 keyterms we would like to take advantage of all the processing power at our disposal. This means we will use the multiprocessing module so that we can gain Nx speed up where N is the number of cpus on a machine.\n",
    "The idea is to\n",
    "* get all the data from all the retailers we have in the data_processing so far,\n",
    "* create a large queue with all that data, and create N processes that draw from the Queue !explain that n is the cpus available\n",
    "* have them each write out a csv with the id of the cpu running it\n",
    "* when its done combine all those results end to end and that is our result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting up\n",
    "\n",
    "Lets first import the packages we will need for this notebook's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# used for creating parallel processing python code\n",
    "import multiprocessing as mp\n",
    "# used for creating our dataframe and outputting a csv\n",
    "import pandas as pd\n",
    "# helps with strings representing time, or python date objects\n",
    "import datetime\n",
    "# lets us create and remove file directories on the computer, or list their contents\n",
    "import os\n",
    "# lets us read JSON files, or lines of JSON\n",
    "import json\n",
    "# regular expressions for subtext extraction\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to collect the data from the various JSON lines files that we created in the course of performing our scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_text_to_process=[]\n",
    "for pth,sub,fls in os.walk(\"./data_processing\"):\n",
    "    for fl in fls:\n",
    "        # make sure we skip the urls file\n",
    "        if \".jl\" in fl and fl != \"all_urls.jl\":\n",
    "            with open(f\"{pth}/{fl}\",\"r\") as phile:\n",
    "                ## add each line of the file to the all_text_to_process\n",
    "                for line in phile:\n",
    "                    # read the json data on line\n",
    "                    content = json.loads(line)\n",
    "                    # add it to the list as a dictionary\n",
    "                    all_text_to_process.append(content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a function that will manipulate our data so that the rest of the code in the notebook has the expected inputs. Each element in `all_text_to_process` is a dictionary with the keys website, text, retailer, but it should also have name, and time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manip_data(e):\n",
    "    # retrieve the part  of the data that has time information\n",
    "    # each website has a section /yymmdd...id_/ in it so we will create a regular expression for that\n",
    "    date_as_string = re.search(r\"(\\d+)id_\",e[\"website\"]).group(1)\n",
    "    # convert to python date time\n",
    "    py_date = datetime.datetime.strptime(date_as_string,\"%Y%m%d%H%M%S\")\n",
    "    # make it into expected international time standard format string \n",
    "    e[\"time\"] = py_date.isoformat(timespec=\"seconds\")\n",
    "    # make name the same as the retailer key, we do this because other code expects a name key to exist\n",
    "    e[\"name\"] = e[\"retailer\"]\n",
    "    # make the website value more understandable and less time specific\n",
    "    e[\"website\"] = re.search(r\"(\\d+)id_/(.*)\",e[\"website\"]).group(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now execute the function on all the elements in our `all_text_to_process`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in all_text_to_process:\n",
    "    manip_data(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a queue out of all the pages we scraped in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "## add the items to queue\n",
    "q = mp.Queue()\n",
    "for element in all_text_to_process:\n",
    "    q.put(element)\n",
    "    \n",
    "## create a place for the data to go\n",
    "if not os.path.exists(\"./data_finished\"):\n",
    "        os.mkdir(\"./data_finished\")\n",
    "        os.mkdir(\"./data_finished/uncombined\")\n",
    "        os.mkdir(\"./data_finished/combined\")\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is where the bulk of the work for this notebook gets done. This is where we actually look for keyterms in the website data. Because this is a lot of website data and a lot of terms we will make it parallel by running on all the cpu cores we have on the machine. The function takes in a list of arguments holding its cpu id, and the queue with the elements to process.\n",
    "\n",
    "The function will add an entry to our `res` list for each webpage we have data for. This helps us track what dates we managed to scrape in visualizations and analysis to come. Without this, we would be left without a clue whether a terms frequency was due to retailers changing the website contents or the wayback machine's scraping schedules. Also the function will add an entry for any of the terms we find in the webpage data.\n",
    "\n",
    "The final output will be a saved csv in the `./data_finished/uncombined` directory labeled with the cpu core id that the function was running on. Said differently, on a 4 cpu machine you will have 4 files `0.csv` - `3.csv` in the uncombined folder. \n",
    "\n",
    "## Tweaks\n",
    "\n",
    "* `slim_text` it's recommended you update the regular expression to suit your natural language processing needs.\n",
    "    * effects: get different values for the column `term_snippets` in the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_element(args):\n",
    "    # i is the cpu id\n",
    "    # qu is the multiprocess queue holding all our website data elements\n",
    "    i,qu = args\n",
    "\n",
    "    # load terms and use it as a global variable available to teh multiprocess function process_element\n",
    "    terms = \"\"\n",
    "    with open(\"./data_processing/terms.txt\",'r') as phile:\n",
    "        terms = phile.read().strip().split(\"\\n\")    \n",
    "    # this is the output result list\n",
    "    res = []\n",
    "    ## loop while the queue isn't empty\n",
    "    while not qu.empty():\n",
    "        # attempt to pull from the queue\n",
    "        element = qu.get()\n",
    "        ## include the scrape record period regardless of term hits, gives context for hits\n",
    "        res.append(dict(name=element[\"name\"],website=element['website'],term = \"NaN\",count = \"NaN\",time = element[\"time\"],term_snippets=\"NaN\"))\n",
    "\n",
    "        ## go through all the terms and test whether they are in the text\n",
    "\n",
    "        for term in terms:\n",
    "            # get occurences of term in website visible text\n",
    "             # make sure we don't count cases like \"flu\" being in \"influentnial\"\n",
    "            # this regex will match the term only if 0 or more non-alphas are surrounding it\n",
    "            count = len(re.findall(f\"[^a-z]*{term}[^a-z]*\",element[\"text\"].lower()))\n",
    "            # if we found alteast 1\n",
    "            if count > 0:\n",
    "                # get up to 100 characters surrouding the term in the website text, re.findall returns a list\n",
    "                slim_text = re.findall(\" .{0,100}\"+ term + \".{0,100} \",element[\"text\"].lower())\n",
    "                ## perform slight processing, to make a single newline separated string\n",
    "                snippets =\"\\n\".join([snip.replace(\"\\n\",\"\") for snip in slim_text])\n",
    "                # add to the results\n",
    "                res.append(dict(name=element[\"name\"],website=element['website'],term = term,count = count,time = element[\"time\"],term_snippets=snippets))\n",
    "    print(\"finished queue,saving\")\n",
    "    # create a pandas dataframe for easy csv export from our res list\n",
    "    df = pd.DataFrame(res)\n",
    "    # don't include an index column, they get annoying\n",
    "    df.to_csv(f\"./data_finished/uncombined/{i}.csv\",index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the function lets set up things for as many different processes running that function as we have cpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of cpus, n\n",
    "cpus = mp.cpu_count()\n",
    "processes =[]\n",
    "# create the processes\n",
    "for i in range(cpus):\n",
    "    # make actual process with the target being our function, and the id of the cpu (0-n) args the queue from above\n",
    "    p = mp.Process(target= process_element,args=([i,q],) )\n",
    "    # start thhe process\n",
    "    p.start()\n",
    "    # add it to our list of processes\n",
    "    processes.append(p)\n",
    "    \n",
    "# wait for each to finish\n",
    "for p in processes:\n",
    "    p.join()\n",
    "    \n",
    "print(\"done\")\n",
    "## combine the separate files together in the combined directory, add to existing dataset if its there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will actually put together our final single csv for visualization and analysis.\n",
    "\n",
    "We will be putting together all the csvs in the uncombined folder, but if we have run this notebook before we should also merge in the final csv from that. This ensures that we don't lose any data if we decide to go through the whole workflow multiple times. We also make sure to de-duplicate our csv data so that we don't see multiple rows with the same retailer, timestamp, and terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make a list containing our pandas dataframes\n",
    "dfs = []\n",
    "# look for all the uncombied csvs\n",
    "for pth,sub,fls in os.walk(\"./data_finished/uncombined\"):\n",
    "    for fl in fls:\n",
    "        df = pd.read_csv(f\"{pth}/{fl}\")\n",
    "        dfs.append(df)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "## see if there's already a complete csv from previous executions of this whole notebook\n",
    "if os.path.exists(\"./data_finished/combined/data.csv\"):\n",
    "    df = pd.read_csv(\"./data_finished/combined/data.csv\")\n",
    "    dfs.append(df)\n",
    "# take a look at our dataframes in case that's interesting to you?    \n",
    "\n",
    "print(dfs)\n",
    "\n",
    "# merge all the dataframes together removing duplicate rows. Note, duplicate means all columns of 2 rows are \"exactly the same\". \n",
    "# if you want to restrict the columns considered for duplication detection look up the details here\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html\n",
    "\n",
    "all_df = pd.concat(dfs).drop_duplicates()\n",
    "\n",
    "# save the total data to the combined directory with the file name as data.csv, and don't include an index column\n",
    "all_df.to_csv(\"./data_finished/combined/data.csv\",index=False)\n",
    "print(\"csv complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
