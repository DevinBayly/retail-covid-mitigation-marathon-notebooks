{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "put in another h1 header explaining that this is the continuation of the previous notebook\n",
    "\n",
    "process the data thats in the data_processing directory against the terms text thats in that directory. Use the multiprocessing module so that we can gain Nx speed up where N is the number of cpus on a machine. The idea is to\n",
    "* get all the data from all the retailers we have in the data_processing so far,\n",
    "* create a large queue with all that data, and create N processes that draw from the Queue !explain that n is the cpus available\n",
    "* have them each write out a csv with the id of the cpu running it\n",
    "* when its done combine all those results end to end and that is our result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!mention why we are installing something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.1.5-cp38-cp38-manylinux1_x86_64.whl (9.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.3 MB 5.8 MB/s eta 0:00:01     |████████                        | 2.3 MB 5.8 MB/s eta 0:00:02     |████████████▊                   | 3.7 MB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /home/dev/miniconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Collecting numpy>=1.15.4\n",
      "  Downloading numpy-1.19.4-cp38-cp38-manylinux2010_x86_64.whl (14.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5 MB 50.3 MB/s eta 0:00:01    |████████████████████▌           | 9.3 MB 50.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2017.2\n",
      "  Downloading pytz-2020.4-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[K     |████████████████████████████████| 509 kB 47.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/dev/miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.14.0)\n",
      "Installing collected packages: numpy, pytz, pandas\n",
      "Successfully installed numpy-1.19.4 pandas-1.1.5 pytz-2020.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to collect the data from teh various JSON lines files that we created in the course of performing our scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "all_text_to_process=[]\n",
    "for pth,sub,fls in os.walk(\"./data_processing\"):\n",
    "    for fl in fls:\n",
    "        # make sure we skip the urls file\n",
    "        if \".jl\" in fl and fl != \"all_urls.jl\":\n",
    "            with open(f\"{pth}/{fl}\",\"r\") as phile:\n",
    "                ## add each line of the file to the all_text_to_process\n",
    "                for line in phile:\n",
    "                    # read the json data on line\n",
    "                    content = json.loads(line)\n",
    "                    # add it to the list as a dictionary\n",
    "                    all_text_to_process.append(content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a function that will manipulate our data so that the rest of the code in the notebook has the expected inputs. Each element in `all_text_to_process` is a dictionary with the keys website, text, retailer, but it should also have name, and time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manip_data(e):\n",
    "    # retrieve the part  of the data that has time information\n",
    "    # each website has a section /yymmdd...id_/ in it so we will create a regular expression for that\n",
    "    date_as_string = re.search(r\"(\\d+)id_\",e[\"website\"]).group(1)\n",
    "    # convert to python date time\n",
    "    py_date = datetime.datetime.strptime(date_as_string,\"%Y%m%d%H%M%S\")\n",
    "    # make it into expected international time standard format string \n",
    "    e[\"time\"] = py_date.isoformat(timespec=\"seconds\")\n",
    "    # make name the same as the retailer key, we do this because other code expects a name key to exist\n",
    "    e[\"name\"] = e[\"retailer\"]\n",
    "    # make the website value more understandable and less time specific\n",
    "    e[\"website\"] = re.search(r\"(\\d+)id_/(.*)\",e[\"website\"]).group(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now execute the function on all the elements in our `all_text_to_process`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in all_text_to_process:\n",
    "    manip_data(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a queue out of all the pages we scraped in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "## add the items to queue\n",
    "q = mp.Queue()\n",
    "for element in all_text_to_process:\n",
    "    q.put(element)\n",
    "    \n",
    "## create a place for the data to go\n",
    "if not os.path.exists(\"./data_finished\"):\n",
    "        os.mkdir(\"./data_finished\")\n",
    "        os.mkdir(\"./data_finished/uncombined\")\n",
    "        os.mkdir(\"./data_finished/combined\")\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is where the bulk of the work for this notebook gets done. This is where we actually look for keyterms in the website data. Because this is a lot of website data and a lot of terms we will make it parallel by running on all the cpu cores we have on the machine. The function takes in a list of arguments holding its cpu id, and the queue with the elements to process.\n",
    "\n",
    "The function will add an entry to our `res` list for each webpage we have data for. This helps us track what dates we managed to scrape in visualizations and analysis to come. Without this, we would be left without a clue whether a terms frequency was due to retailers changing the website contents or the wayback machine's scraping schedules. Also the function will add an entry for any of the terms we find in the webpage data.\n",
    "\n",
    "The final output will be a saved csv in the `./data_finished/uncombined` directory labeled with the cpu core id that the function was running on. Said differently, on a 4 cpu machine you will have 4 files `0.csv` - `3.csv` in the uncombined folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_element(args):\n",
    "    # i is the cpu id\n",
    "    # qu is the multiprocess queue holding all our website data elements\n",
    "    i,qu = args\n",
    "\n",
    "    # load terms and use it as a global variable available to teh multiprocess function process_element\n",
    "    terms = \"\"\n",
    "    with open(\"./data_processing/terms.txt\",'r') as phile:\n",
    "        terms = phile.read().strip().split(\"\\n\")    \n",
    "    # this is the output result list\n",
    "    res = []\n",
    "    ## loop while the queue isn't empty\n",
    "    while not qu.empty():\n",
    "        # attempt to pull from the queue\n",
    "        element = qu.get()\n",
    "        ## include the scrape record period regardless of term hits, gives context for hits\n",
    "        res.append(dict(name=element[\"name\"],website=element['website'],term = \"NaN\",count = \"NaN\",time = element[\"time\"],term_snippets=\"NaN\"))\n",
    "\n",
    "        ## go through all the terms and test whether they are in the text\n",
    "\n",
    "        for term in terms:\n",
    "            # get occurences of term in website visible text\n",
    "            count = element[\"text\"].lower().count(term.lower().strip())\n",
    "            # if we found alteast 1\n",
    "            if count > 0:\n",
    "                # get up to 100 characters surrouding the term in the website text, re.findall returns a list\n",
    "                slim_text = re.findall(\" .{0,100}\"+ term + \".{0,100} \",element[\"text\"].lower())\n",
    "                ## perform slight processing, to make a single newline separated string\n",
    "                snippets =\"\\n\".join([snip.replace(\"\\n\",\"\") for snip in slim_text])\n",
    "                # add to the results\n",
    "                res.append(dict(name=element[\"name\"],website=element['website'],term = term,count = count,time = element[\"time\"],term_snippets=snippets))\n",
    "    print(\"finished queue,saving\")\n",
    "    # create a pandas dataframe for easy csv export from our res list\n",
    "    df = pd.DataFrame(res)\n",
    "    # don't include an index column, they get annoying\n",
    "    df.to_csv(f\"./data_finished/uncombined/{i}.csv\",index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the function lets set up things for as many different processes running that function as we have cpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished queue,saving\n",
      "finished queue,saving\n",
      "finished queue,saving\n",
      "finished queue,saving\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# get number of cpus, n\n",
    "cpus = mp.cpu_count()\n",
    "processes =[]\n",
    "# create the processes\n",
    "for i in range(cpus):\n",
    "    # make actual process with the target being our function, and the id of the cpu (0-n) args the queue from above\n",
    "    p = mp.Process(target= process_element,args=([i,q],) )\n",
    "    # start thhe process\n",
    "    p.start()\n",
    "    # add it to our list of processes\n",
    "    processes.append(p)\n",
    "    \n",
    "# wait for each to finish\n",
    "for p in processes:\n",
    "    p.join()\n",
    "    \n",
    "print(\"done\")\n",
    "## combine the separate files together in the combined directory, add to existing dataset if its there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will actually put together our final single csv for visualization and analysis.\n",
    "\n",
    "We will be putting together all the csvs in the uncombined folder, but if we have run this notebook before we should also merge in the final csv from that. This ensures that we don't lose any data if we decide to go through the whole workflow multiple times. We also make sure to de-duplicate our csv data so that we don't see multiple rows with the same retailer, timestamp, and terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[           name                                            website  \\\n",
      "0     fredmeyer                 https://www.fredmeyerjewelers.com/   \n",
      "1     fredmeyer                 https://www.fredmeyerjewelers.com/   \n",
      "2     fredmeyer                 https://www.fredmeyerjewelers.com/   \n",
      "3     fredmeyer                 https://www.fredmeyerjewelers.com/   \n",
      "4     fredmeyer                 https://www.fredmeyerjewelers.com/   \n",
      "...         ...                                                ...   \n",
      "1088   samsclub  https://corporate.samsclub.com/member-update-s...   \n",
      "1089   samsclub  https://corporate.samsclub.com/member-update-s...   \n",
      "1090   samsclub  https://corporate.samsclub.com/member-update-s...   \n",
      "1091   samsclub  https://corporate.samsclub.com/member-update-s...   \n",
      "1092   samsclub  https://corporate.samsclub.com/member-update-s...   \n",
      "\n",
      "                    term  count                 time  \\\n",
      "0                    NaN    NaN  2020-09-03T19:39:49   \n",
      "1                  covid    1.0  2020-09-03T19:39:49   \n",
      "2               covid-19    1.0  2020-09-03T19:39:49   \n",
      "3               customer    1.0  2020-09-03T19:39:49   \n",
      "4                    NaN    NaN  2020-09-09T17:40:56   \n",
      "...                  ...    ...                  ...   \n",
      "1088         for seniors    1.0  2020-09-02T01:49:36   \n",
      "1089  healthcare workers    1.0  2020-09-02T01:49:36   \n",
      "1090    first responders    2.0  2020-09-02T01:49:36   \n",
      "1091               carts    1.0  2020-09-02T01:49:36   \n",
      "1092            customer    4.0  2020-09-02T01:49:36   \n",
      "\n",
      "                                          term_snippets  \n",
      "0                                                   NaN  \n",
      "1      --sep-- --sep-- --sep--covid-19: we are avail...  \n",
      "2      --sep-- --sep-- --sep--covid-19: we are avail...  \n",
      "3                                                   NaN  \n",
      "4                                                   NaN  \n",
      "...                                                 ...  \n",
      "1088   club introduces special shopping hours, conci...  \n",
      "1089   sunday from 8 a.m. – 10 a.m. and we’re expand...  \n",
      "1090   senior hours for senior and at-risk members a...  \n",
      "1091   in our clubs. lastly, we’re making shopping c...  \n",
      "1092   local options. also, please call in refills a...  \n",
      "\n",
      "[1093 rows x 6 columns],            name                                            website      term  \\\n",
      "0     fredmeyer                 https://www.fredmeyerjewelers.com/       NaN   \n",
      "1     fredmeyer                 https://www.fredmeyerjewelers.com/     covid   \n",
      "2     fredmeyer                 https://www.fredmeyerjewelers.com/  covid-19   \n",
      "3     fredmeyer                 https://www.fredmeyerjewelers.com/  customer   \n",
      "4        kroger                       https://www.kroger.com/d/flu       NaN   \n",
      "...         ...                                                ...       ...   \n",
      "1221   samsclub  https://www.samsclub.com/c/health/10230119?xid...      high   \n",
      "1222   samsclub  https://www.samsclub.com/c/health/10230119?xid...      test   \n",
      "1223   samsclub  https://www.samsclub.com/c/health/10230119?xid...   testing   \n",
      "1224   samsclub  https://www.samsclub.com/c/health/10230119?xid...   routine   \n",
      "1225   samsclub  https://www.samsclub.com/c/health/10230119?xid...  symptoms   \n",
      "\n",
      "      count                 time  \\\n",
      "0       NaN  2020-09-10T20:43:03   \n",
      "1       1.0  2020-09-10T20:43:03   \n",
      "2       1.0  2020-09-10T20:43:03   \n",
      "3       1.0  2020-09-10T20:43:03   \n",
      "4       NaN  2020-09-10T23:49:22   \n",
      "...     ...                  ...   \n",
      "1221    2.0  2020-09-06T04:54:43   \n",
      "1222    2.0  2020-09-06T04:54:43   \n",
      "1223    1.0  2020-09-06T04:54:43   \n",
      "1224    1.0  2020-09-06T04:54:43   \n",
      "1225    1.0  2020-09-06T04:54:43   \n",
      "\n",
      "                                          term_snippets  \n",
      "0                                                   NaN  \n",
      "1      --sep-- --sep-- --sep--covid-19: we are avail...  \n",
      "2      --sep-- --sep-- --sep--covid-19: we are avail...  \n",
      "3                                                   NaN  \n",
      "4                                                   NaN  \n",
      "...                                                 ...  \n",
      "1221   club private label--sep-- offers a wide varie...  \n",
      "1222   health supplies:--sep-- pregnancy and fertili...  \n",
      "1223   products have the highest quality ingredients...  \n",
      "1224   at sam’s club®, we’re here to help. one posit...  \n",
      "1225                                                NaN  \n",
      "\n",
      "[1226 rows x 6 columns],            name                                          website  \\\n",
      "0     fredmeyer               https://www.fredmeyerjewelers.com/   \n",
      "1     fredmeyer               https://www.fredmeyerjewelers.com/   \n",
      "2     fredmeyer               https://www.fredmeyerjewelers.com/   \n",
      "3     fredmeyer               https://www.fredmeyerjewelers.com/   \n",
      "4        kroger                     https://www.kroger.com/d/flu   \n",
      "...         ...                                              ...   \n",
      "1129   samsclub  https://corporate.samsclub.com/newsroom/archive   \n",
      "1130   samsclub  https://corporate.samsclub.com/newsroom/archive   \n",
      "1131   samsclub  https://corporate.samsclub.com/newsroom/archive   \n",
      "1132   samsclub  https://corporate.samsclub.com/newsroom/archive   \n",
      "1133   samsclub  https://corporate.samsclub.com/newsroom/archive   \n",
      "\n",
      "                term  count                 time  \\\n",
      "0                NaN    NaN  2020-09-05T01:13:01   \n",
      "1              covid    1.0  2020-09-05T01:13:01   \n",
      "2           covid-19    1.0  2020-09-05T01:13:01   \n",
      "3           customer    1.0  2020-09-05T01:13:01   \n",
      "4                NaN    NaN  2020-09-03T21:24:11   \n",
      "...              ...    ...                  ...   \n",
      "1129         require    1.0  2020-09-09T02:05:05   \n",
      "1130            high    1.0  2020-09-09T02:05:05   \n",
      "1131        pandemic    2.0  2020-09-09T02:05:05   \n",
      "1132  face coverings    1.0  2020-09-09T02:05:05   \n",
      "1133        customer    1.0  2020-09-09T02:05:05   \n",
      "\n",
      "                                          term_snippets  \n",
      "0                                                   NaN  \n",
      "1      --sep-- --sep-- --sep--covid-19: we are avail...  \n",
      "2      --sep-- --sep-- --sep--covid-19: we are avail...  \n",
      "3                                                   NaN  \n",
      "4                                                   NaN  \n",
      "...                                                 ...  \n",
      "1129   simple step to help keep you safe: walmart an...  \n",
      "1130                  mother’s day is may 10: igi ce...  \n",
      "1131                  throughout this pandemic, your...  \n",
      "1132   simple step to help keep you safe: walmart an...  \n",
      "1133   next step to protect you, our customers and our   \n",
      "\n",
      "[1134 rows x 6 columns],            name                                            website  \\\n",
      "0     fredmeyer                 https://www.fredmeyerjewelers.com/   \n",
      "1     fredmeyer                 https://www.fredmeyerjewelers.com/   \n",
      "2     fredmeyer                 https://www.fredmeyerjewelers.com/   \n",
      "3     fredmeyer                 https://www.fredmeyerjewelers.com/   \n",
      "4     fredmeyer                    https://www.fredmeyer.com/d/flu   \n",
      "...         ...                                                ...   \n",
      "1149   samsclub  https://corporate.samsclub.com/member-update-s...   \n",
      "1150   samsclub  https://corporate.samsclub.com/member-update-s...   \n",
      "1151   samsclub  https://corporate.samsclub.com/member-update-s...   \n",
      "1152   samsclub  https://corporate.samsclub.com/member-update-s...   \n",
      "1153   samsclub  https://corporate.samsclub.com/member-update-s...   \n",
      "\n",
      "                    term  count                 time  \\\n",
      "0                    NaN    NaN  2020-09-04T19:26:17   \n",
      "1                  covid    1.0  2020-09-04T19:26:17   \n",
      "2               covid-19    1.0  2020-09-04T19:26:17   \n",
      "3               customer    1.0  2020-09-04T19:26:17   \n",
      "4                    NaN    NaN  2020-09-07T21:34:10   \n",
      "...                  ...    ...                  ...   \n",
      "1149         for seniors    1.0  2020-09-09T02:05:10   \n",
      "1150  healthcare workers    1.0  2020-09-09T02:05:10   \n",
      "1151    first responders    2.0  2020-09-09T02:05:10   \n",
      "1152               carts    1.0  2020-09-09T02:05:10   \n",
      "1153            customer    4.0  2020-09-09T02:05:10   \n",
      "\n",
      "                                          term_snippets  \n",
      "0                                                   NaN  \n",
      "1      --sep-- --sep-- --sep--covid-19: we are avail...  \n",
      "2      --sep-- --sep-- --sep--covid-19: we are avail...  \n",
      "3                                                   NaN  \n",
      "4                                                   NaN  \n",
      "...                                                 ...  \n",
      "1149   club introduces special shopping hours, conci...  \n",
      "1150   sunday from 8 a.m. – 10 a.m. and we’re expand...  \n",
      "1151   senior hours for senior and at-risk members a...  \n",
      "1152   in our clubs. lastly, we’re making shopping c...  \n",
      "1153   local options. also, please call in refills a...  \n",
      "\n",
      "[1154 rows x 6 columns]]\n",
      "csv complete\n"
     ]
    }
   ],
   "source": [
    "# make a list containing our pandas dataframes\n",
    "dfs = []\n",
    "# look for all the uncombied csvs\n",
    "for pth,sub,fls in os.walk(\"./data_finished/uncombined\"):\n",
    "    for fl in fls:\n",
    "        df = pd.read_csv(f\"{pth}/{fl}\")\n",
    "        dfs.append(df)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "## see if there's already a complete csv from previous executions of this whole notebook\n",
    "if os.path.exists(\"./data_finished/combined/data.csv\"):\n",
    "    df = pd.read_csv(\"./data_finished/combined/data.csv\")\n",
    "    dfs.append(df)\n",
    "# take a look at our dataframes in case that's interesting to you?    \n",
    "\n",
    "print(dfs)\n",
    "\n",
    "# merge all the dataframes together removing duplicate rows. Note, duplicate means all columns of 2 rows are \"exactly the same\". \n",
    "# if you want to restrict the columns considered for duplication detection look up the details here\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html\n",
    "\n",
    "all_df = pd.concat(dfs).drop_duplicates()\n",
    "\n",
    "# save the total data to the combined directory with the file name as data.csv, and don't include an index column\n",
    "all_df.to_csv(\"./data_finished/combined/data.csv\",index=False)\n",
    "print(\"csv complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
