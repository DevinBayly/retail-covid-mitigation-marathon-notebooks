{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time travel Time!\n",
    "\n",
    "Now we will use all our collected links to get data from the way back machine. \n",
    "\n",
    "If you haven't heard of the wayback machine before it is an ongoing subproject of the Internet Archive. They go out and collect snapshots of what websites looked like over time. In total they have the largest collection of accessible website history on the planet. Perhaps google beats them out, but there's no way to leverage their collection for free (as far as I know?).\n",
    "\n",
    "It should be noted that not all the links we found in our gathering step will have data in the wayback machine. This is for two reasons at least. \n",
    "\n",
    "One, websites are super fluid things always getting changes made to them, and subpages get added frequently even outside of a pandemic emergency. This means that as of the time I write this our links are reflections of retailers pages in December 2020. Certain pages may simply not exist if we look back to January 2020. It's an assumption that the majority of worthwhile material out there representing a retailers communications about covid mitigations won't have been created and then removed before this date since the pandemic isn't over. This means that I have some confidence that pages with pandemic information on them will have a legacy that we can track over the last 12 months.  \n",
    "\n",
    "Two, the wayback machine has limited resources. They manage an incredible amount of scraping, but some of the links that I've collected simply don't have historical snapshots on the wayback machine. Of the 120 that I collected most recently, about 5 of them had 0 snapshots. I believe that this is a sufficiently small amount of missed data, and it doesn't indicate that the methods of collection are faulty. That said, some pages have more than 0 snapshots, but nothing like daily coverage. To get around these issues we would have to effectively take on teh place of teh wayback machine and run daily scrapes going forward in time with a collection of relevant links. That is outside the scope of our current project. Hopefully these restrictions won't prevent us from identifying interesting trends in what terms showed up on what retailer pages at what times. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first lets import all the needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our scraper\n",
    "import scrapy\n",
    "# regular expressions library, useful for extracting text\n",
    "import re\n",
    "# shows progress bars\n",
    "from tqdm import tqdm\n",
    "# parallel programming, lets us write faster code\n",
    "from multiprocessing import Process,Queue\n",
    "# Crawler Process used to start a scrapy spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "# beautiful soup for parsing webpages and getting visible text\n",
    "from bs4 import BeautifulSoup as bs\n",
    "# basic web requests, for getting list of wayback snapshots for page if any \n",
    "import requests as rq\n",
    "# url encoder, needed for certain wayback snapshots\n",
    "from requests.utils import quote\n",
    "# help us read json and json line files\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a function which returns a spider class for the crawler process to start. Instead of defining the class outside the function this way we can provide variables that the class can operate on such as `url`, `retailer`, `from_date`, and `to_date`. Respectively these are the website url, the retailer name, the yyyymmdd format start of the date range we want, and yyyymmdd format end of the date range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_make_spider(url,retailer,from_date,to_date):\n",
    "    # catch error when wayback has no snapshots for this page in the date range\n",
    "    try:\n",
    "        # encode the url safely, this makes it possible to pass urls that have queries in them\n",
    "        clean_url = quote(url,safe=\"\")\n",
    "        # make a request to get the list of historical snapshots\n",
    "        res = rq.get(f'https://web.archive.org/cdx/search/cdx?url={clean_url}&from={from_date}&to={to_date}'\n",
    "                     '&output=json&fl=timestamp,original,statuscode,digest')\n",
    "        # this returns a json array with a header line (think csv columns)\n",
    "        # the first line of the returned content isn't data \n",
    "        header= json.loads(res.content)[0]\n",
    "        # get the rest of the data, we only really care about whats in the first column, the timestamps\n",
    "        res_data = json.loads(res.content)[1:]\n",
    "    except Exception as e:\n",
    "        # problem is likeely just that there was no wayback machine scrapes for this page in the date range\n",
    "        print(\"err problem with url\",e)\n",
    "        print(res.content)\n",
    "        # log it to a file so we can tell which pages didn't get data\n",
    "        with open(\"problem_log\",\"a\") as phile:\n",
    "            phile.write(f\"{e} {url}\\n\")\n",
    "        return\n",
    "    # we now construct a list of the timestamps that are all atleast 1 day apart\n",
    "    # we don't want to scrape at any granualarity less that 1 day because it increases the number of results dramatically\n",
    "    # and the pages may not have anything of sigificance in terms of changes\n",
    "    single_days = []\n",
    "    # this holds the exact timestamp to give to the wayback machine to return a page at that date\n",
    "    long_form = []\n",
    "    # loop over the data\n",
    "    for r in res_data:\n",
    "        # the 8 encompases the date info in the string yyyymmdd without going into hours minutes and seconds \n",
    "        single_date = r[0][:8]\n",
    "        # if we haven't already added the shortened day to the single days, do so and add the long form version of the timestamp \n",
    "        if not single_date in single_days:\n",
    "            single_days.append(single_date)\n",
    "            long_form.append(r[0])\n",
    "    \n",
    "    # urls gets used in start_requests, templated out for each of our long form timestamps\n",
    "    urls = [f'http://web.archive.org/web/{timestamp}id_/{url}' for timestamp in long_form]\n",
    "    class MySpider(scrapy.Spider):\n",
    "        #  spider definition\n",
    "        name='retail'\n",
    "        # auto throttle so the server isn't overloaded from our collections\n",
    "        # this helps them not want to kick us off\n",
    "        # delay our downloads by 1 second also\n",
    "        # allow max 3 requests to go to waybackmachine, this can go as high as 10 I think, but\n",
    "        # again they might decide to kick you if you make too many simultaneous requests\n",
    "        custom_settings = {\n",
    "            'AUTOTHROTTLE_ENABLED': True,\n",
    "            'AUTOTHROTTLE_DEBUG': True,\n",
    "            'DOWNLOAD_DELAY':1,\n",
    "            'AUTOTHROTTLE_TARGET_CONCURRENCY':3,\n",
    "        }\n",
    "        # make a request for each appropriate timestamp, these will get queued and processed atmost 3 at a time \n",
    "        def start_requests(self):\n",
    "            for url in urls:\n",
    "                yield scrapy.Request(url=url,callback=self.parse)\n",
    "        # for each webpage we get back save the visual information as a line in our results log (described lower down)\n",
    "        def parse(self,response):\n",
    "            # convert to bs\n",
    "            soup = bs(response.body)\n",
    "            # put a specific separator between each of the html elements visual text\n",
    "            text = soup.get_text('--sep--')\n",
    "            yield {\"website\":response.url,\"text\":text,\"retailer\":retailer}\n",
    "            \n",
    "\n",
    "    return MySpider\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a function that actually makes use of the function we created above.\n",
    "\n",
    "run_spider takes a websie url, retailer name, from_date, and to_date. it will then activate a spider to start crawling the wayback machine.\n",
    "\n",
    "Note the FEEDS. This specifies where the output should go when we get data from the spider. For each retailer we will store the raw website data in the `./data_processing` folder under another folder with the retailer name. The actual file will be named `timed_scrapes_{from_date}_{to_date}.jl` with the from_date and to_date info in it. \n",
    "\n",
    "The final notebook will look through the data_processing folder for our result files and put them all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_spider(url,retailer,from_date,to_date):\n",
    "    # create a CrawlerProcess with certain settings\n",
    "    # the f\"...\" string is a formatted string allowing us to put variables in a specific positions in the string\\\n",
    "    \n",
    "    process = CrawlerProcess({\n",
    "            'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1), retail covid research',\n",
    "            \"FEEDS\":{\n",
    "                f\"./data_processing/{retailer}/timed_scrapes_{from_date}_{to_date}.jl\":{\"format\":\"jsonlines\"},\n",
    "            }\n",
    "    })\n",
    "    # call the url_make_spider function which sets up our spider class with url,retailer,from_date, and to_date\n",
    "    myspider_class = url_make_spider(url,retailer,from_date,to_date)\n",
    "    # provide the spider class to the crawl process\n",
    "    process.crawl(myspider_class)\n",
    "    # go ahead and start\n",
    "    process.start() \n",
    "    # nothing happens after this until the crawling is finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will actually start collecting data from the wayback machine.\n",
    "\n",
    "first we get all our values of retailer websites identified in current day to hold important pandemic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620\n"
     ]
    }
   ],
   "source": [
    "websites = set()\n",
    "with open(\"./data_processing/all_urls.jl\") as phile:\n",
    "    for line in phile:\n",
    "        j_ob = json.loads(line)\n",
    "        websites.add((j_ob[\"website\"],j_ob[\"retailer\"]))\n",
    "\n",
    "print(len(websites))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we establish the bounds of our time range that we care about. Feel free to modify these values whenever you re run this notebook. the pattern to follow is yyyymmdd. so we use 4 numbers for the year, 2 for the month, and 2 for the days. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from_date = \"20200110\"\n",
    "to_date =\"20201230\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we go through this entire list creating a history grabbing spider to go through. Again if your computer becomes sluggish because of the amount of output feel free to remove the `#` in front of `#%%capture` and none of the output will come from the cell while it runs. Also note, this is not parallelized code, because we don't really want to hammer the wayback machine with requests. It's like a really wise senior citizen, we can learn lots if we are patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# loop and setup a progress bar with tqdm\n",
    "# each value in websites is like this [website,retailer], so we can destructure to website, retailer in the for loop\n",
    "for website,retailer in tqdm(websites):\n",
    "    p = Process(target=run_spider,args=(website,retailer,from_date,to_date))\n",
    "    p.start()\n",
    "    p.join()\n",
    "#run_spider(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
